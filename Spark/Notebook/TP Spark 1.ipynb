{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------\n",
    "# <div style=\"text-align:center;\">TP Spark 1 : notes perso</div>\n",
    "-------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word count avec un RDD\n",
    "---------------\n",
    "\n",
    "### Lire un fichier de données non structurées via un RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://137.194.75.81:4041\n",
       "SparkContext available as 'sc' (version = 2.4.4, master = local[*], app id = local-1571126695420)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[String] = /home/p5hngk/Downloads/GitHub/INF_729---Introduction_au_framework_Hadoop/cours-spark-telecom-master/README.md MapPartitionsRDD[1] at textFile at <console>:25\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.textFile(\"/home/p5hngk/Downloads/GitHub/INF_729---Introduction_au_framework_Hadoop/cours-spark-telecom-master/README.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afficher les 5 premières lignes du fichier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Cours sur Spark\n",
      "\n",
      "Vous trouverez ici toutes les ressources pour la partie travaux pratiques du cours sur Spark donné à Télécom pour le Mastère Spécialisé Big Data.\n",
      "\n",
      "Il y a 3 fichiers de TP:\n"
     ]
    }
   ],
   "source": [
    "rdd.take(5).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word count\n",
    "\n",
    "Comptons désormais le nombre de mots dans le fichier et affichons les 10 premières lignes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(à,1)\n",
      "(pouvoir,1)\n",
      "(Cours,1)\n",
      "(pour,3)\n",
      "(des,1)\n",
      "(utilisera,1)\n",
      "(nécessaire,1)\n",
      "(plus,1)\n",
      "(deux,2)\n",
      "(lors,1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "wordCount: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[4] at reduceByKey at <console>:29\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wordCount = rdd\n",
    "  .flatMap(line => line.split(\" \"))\n",
    "  .map(word => (word, 1))\n",
    "  .reduceByKey((i, j) => i + j)\n",
    "\n",
    "wordCount.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: Long = 12\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Cours sur Spark\n",
      "\n",
      "Vous trouverez ici toutes les ressources pour la partie travaux pratiques du cours sur Spark donné à Télécom pour le Mastère Spécialisé Big Data.\n",
      "\n",
      "Il y a 3 fichiers de TP:\n"
     ]
    }
   ],
   "source": [
    "rdd.take(5).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flattenedRdd: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[5] at flatMap at <console>:26\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val flattenedRdd = rdd.flatMap(line => line.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: Long = 109\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattenedRdd.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\n",
      "Cours\n",
      "sur\n",
      "Spark\n",
      "\n",
      "Vous\n",
      "trouverez\n",
      "ici\n",
      "toutes\n",
      "les\n"
     ]
    }
   ],
   "source": [
    "flattenedRdd.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Digression : types des variables\n",
    "\n",
    "Pour info, vous pouvez obtenir le type d'un objet en tapant simplement le nom de l'objet dans le shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res8: org.apache.spark.rdd.RDD[String] = /home/p5hngk/Downloads/GitHub/INF_729---Introduction_au_framework_Hadoop/cours-spark-telecom-master/README.md MapPartitionsRDD[1] at textFile at <console>:25\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res9: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[5] at flatMap at <console>:26\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattenedRdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reducedRdd: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[7] at reduceByKey at <console>:26\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val reducedRdd = flattenedRdd.map(word => (word, 1)).reduceByKey((i, j) => i + j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res11: Long = 79\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reducedRdd.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(à,1)\n",
      "(pouvoir,1)\n",
      "(Cours,1)\n",
      "(pour,3)\n",
      "(des,1)\n",
      "(utilisera,1)\n",
      "(nécessaire,1)\n",
      "(plus,1)\n",
      "(deux,2)\n",
      "(lors,1)\n"
     ]
    }
   ],
   "source": [
    "reducedRdd.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mots les plus fréquents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(,4)\n",
      "(sur,4)\n",
      "(Spark,4)\n",
      "(et,4)\n",
      "(pour,3)\n",
      "(de,3)\n",
      "(-,3)\n",
      "(qui,3)\n",
      "(fichiers,3)\n",
      "(deux,2)\n"
     ]
    }
   ],
   "source": [
    "// Obtenir les mots les plus fréquents\n",
    "wordCount\n",
    "  // wordAndCount est un tuple, on accède à son 2e élément, le count, via ._2\n",
    "  .sortBy(wordAndCount => wordAndCount._2, ascending=false)\n",
    "  .take(10)\n",
    "  .foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(,4)\n",
      "(sur,4)\n",
      "(spark,4)\n",
      "(et,4)\n",
      "(pour,3)\n",
      "(de,3)\n",
      "(-,3)\n",
      "(qui,3)\n",
      "(fichiers,3)\n",
      "(deux,2)\n"
     ]
    }
   ],
   "source": [
    "wordCount\n",
    "  .map(wordAndCount => (wordAndCount._1.toLowerCase, wordAndCount._2))\n",
    "  .reduceByKey((i, j) => i + j)\n",
    "  .sortBy(wordAndCount => wordAndCount._2, ascending=false)\n",
    "  .take(10)\n",
    "  .foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word count avec un DataFrame\n",
    "-------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant qu'on a vu les bases de la manipulation d'un RDD, regardons ce que ça donne avec un DataFrame.\n",
    "\n",
    "Pour rappel, on doit passer par un SparkSession pour pouvoir utiliser des DataFrames (et Datasets). **Dans le spark-shell** un SparkSession est automatiquement créé pour vous et est accessible en tapant `spark.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [value: string]\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.text(\"/home/p5hngk/Downloads/GitHub/INF_729---Introduction_au_framework_Hadoop/cours-spark-telecom-master/README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|   # Cours sur Spark|\n",
      "|                    |\n",
      "|Vous trouverez ic...|\n",
      "|                    |\n",
      "|Il y a 3 fichiers...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|value                                                                                                                                            |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|# Cours sur Spark                                                                                                                                |\n",
      "|                                                                                                                                                 |\n",
      "|Vous trouverez ici toutes les ressources pour la partie travaux pratiques du cours sur Spark donné à Télécom pour le Mastère Spécialisé Big Data.|\n",
      "|                                                                                                                                                 |\n",
      "|Il y a 3 fichiers de TP:                                                                                                                         |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Par défaut, l'affichage est tronqué, pour afficher toute la ligne :\n",
    "df.show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordCountDF: org.apache.spark.sql.DataFrame = [word: string, count: bigint]\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wordCountDF = df\n",
    "  .withColumn(\"word\", split($\"value\", \" \"))\n",
    "  .withColumn(\"word\", explode($\"word\")) // équivalent du flatten plus haut\n",
    "  .groupBy(\"word\")\n",
    "  .count // crée une colonne \"count\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|     word|count|\n",
      "+---------+-----+\n",
      "|         |    4|\n",
      "|      sur|    4|\n",
      "|       et|    4|\n",
      "|    Spark|    4|\n",
      "|        -|    3|\n",
      "|     pour|    3|\n",
      "|      qui|    3|\n",
      "|       de|    3|\n",
      "| fichiers|    3|\n",
      "|trouverez|    2|\n",
      "+---------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordCountDF \n",
    "    .orderBy($\"count\".desc)\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      word|count|\n",
      "+----------+-----+\n",
      "|     spark|    4|\n",
      "|          |    4|\n",
      "|       sur|    4|\n",
      "|        et|    4|\n",
      "|       qui|    3|\n",
      "|      pour|    3|\n",
      "|  fichiers|    3|\n",
      "|         -|    3|\n",
      "|        de|    3|\n",
      "|ressources|    2|\n",
      "+----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Il faut maintenant passer les mots en minuscules et agréger les résultats :\n",
    "\n",
    "wordCountDF\n",
    "    .withColumn(\"word\", lower($\"word\"))\n",
    "    .groupBy(\"word\")\n",
    "    .agg(sum($\"count\") as \"count\") // agg : le point d'entrée des fonctions d'agrégation\n",
    "    .orderBy($\"count\".desc)\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      word|count|\n",
      "+----------+-----+\n",
      "|          |    4|\n",
      "|       sur|    4|\n",
      "|     Spark|    4|\n",
      "|        et|    4|\n",
      "|      pour|    3|\n",
      "|       qui|    3|\n",
      "|        de|    3|\n",
      "|         -|    3|\n",
      "|  fichiers|    3|\n",
      "|ressources|    2|\n",
      "+----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Autre solution équivalente :\n",
    "\n",
    "wordCountDF\n",
    "    .withColumn(\"word\", lower($\"word\"))\n",
    "    .groupBy(\"word\")\n",
    "    .count\n",
    "\n",
    "wordCountDF\n",
    "    .orderBy($\"count\".desc)\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus : Persistance\n",
    "----------------------\n",
    "*Kill du noyau puis relance d'un nouveau.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(à,1)\n",
      "(pouvoir,1)\n",
      "(Cours,1)\n",
      "(pour,3)\n",
      "(des,1)\n",
      "(utilisera,1)\n",
      "(nécessaire,1)\n",
      "(plus,1)\n",
      "(deux,2)\n",
      "(lors,1)\n",
      "(Big,1)\n",
      "(Mastère,1)\n",
      "(pratiques,1)\n",
      "(les,2)\n",
      "(ce,1)\n",
      "(installer,1)\n",
      "(TP:,1)\n",
      "(,4)\n",
      "(documentation,1)\n",
      "(projet,,1)\n",
      "(donné,1)\n",
      "(Scala.,1)\n",
      "(tout,1)\n",
      "(cours,1)\n",
      "(qu'on,1)\n",
      "(Data.,1)\n",
      "(Ce,1)\n",
      "(trouverez,2)\n",
      "(sont,1)\n",
      "(sa,1)\n",
      "(ressources,2)\n",
      "(rassemble,1)\n",
      "(mais,1)\n",
      "(autres,1)\n",
      "(également,2)\n",
      "(:,1)\n",
      "(lire,1)\n",
      "(séances,1)\n",
      "(toutes,1)\n",
      "(Spécialisé,1)\n",
      "(TP.,1)\n",
      "(sur,4)\n",
      "(est,2)\n",
      "([TP_3_machine_learning_avec_spark.md](TP_3_machine_learning_avec_spark.md),1)\n",
      "(fichiers.,1)\n",
      "(la,1)\n",
      "(Vous,2)\n",
      "(ici,1)\n",
      "(Spark,4)\n",
      "(ces,1)\n",
      "(que,1)\n",
      "(créer,1)\n",
      "(Télécom,1)\n",
      "(généralités,1)\n",
      "([spark_notes.md](spark_notes.md),1)\n",
      "(a,1)\n",
      "(travaux,1)\n",
      "(de,3)\n",
      "(y,1)\n",
      "(le,1)\n",
      "(recommandé,1)\n",
      "(et,4)\n",
      "(#,1)\n",
      "([TP_1_spark_shell_et_word_count.md](TP_1_spark_shell_et_word_count.md),1)\n",
      "(faire,1)\n",
      "([TP_2_projet_et_pre_processings.md](TP_2_projet_et_pre_processings.md),1)\n",
      "(3,1)\n",
      "(décrit,1)\n",
      "(un,1)\n",
      "(du,1)\n",
      "(-,3)\n",
      "(partie,1)\n",
      "(quelques,1)\n",
      "(tourner,1)\n",
      "([setup.md](setup.md),1)\n",
      "(Il,2)\n",
      "(qui,3)\n",
      "(comment,1)\n",
      "(fichiers,3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[85] at flatMap at <console>:27\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.textFile(\"/home/p5hngk/Downloads/GitHub/INF_729---Introduction_au_framework_Hadoop/cours-spark-telecom-master/README.md\").flatMap(line => line.split(\" \"))\n",
    "\n",
    "rdd.map(word => (word, 1)).reduceByKey((i, j) => i + j).collect.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allez ensuite dans l'UI de spark, à l'adresse suivante : \n",
    "http://localhost:4040/jobs/\n",
    "\n",
    "\n",
    "**<span style=\"color: red\">ATTENTION</span>**, pour pouvoir accéder à l'adresse ci-dessus, il faut avoir lancé un `spark-shell` dans un terminal, avec le job de la cellule ci-dessus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si l'on relance la même commande et qu'on regarde les mêmes visualisations et métriques, on obtiendra à peu de chose près les mêmes résultats. Cela signifie **qu'à chaque fois** on lit le fichier texte *README* et on effectue la partie `.flatMap(line => line.split(\" \"))`. Ici c'est très rapide car le fichier est petit et sauvegardé sur notre machine. Mais s'il s'agissait d'un ou plusieurs fichiers de plusieurs giga stockés dans le cloud ? La lecture prendrait beaucoup plus de temps, tout comme le `flatMap`. S'il s'agissait d'un dataset d'entraînement pour un modèle de machine learning sur lequel on doit fait plusieurs passes pour optimiser les hyperparamètres ? Ça serait lent et long.\n",
    "\n",
    "Pour gérer ce problème, Spark donne la possibilité de **persister le RDD/DataFrame** (que ce soit en mémoire, sur disque, ou un mix des deux) i.e. de sauvegarder l'état actuel du RDD/DataFrame pour ne pas avoir à le recomputer à chaque fois :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res26: Long = 109\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.persist\n",
    "rdd.count // force l'exécution et en particulier la persistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(à,1)\n",
      "(pouvoir,1)\n",
      "(Cours,1)\n",
      "(pour,3)\n",
      "(des,1)\n",
      "(utilisera,1)\n",
      "(nécessaire,1)\n",
      "(plus,1)\n",
      "(deux,2)\n",
      "(lors,1)\n",
      "(Big,1)\n",
      "(Mastère,1)\n",
      "(pratiques,1)\n",
      "(les,2)\n",
      "(ce,1)\n",
      "(installer,1)\n",
      "(TP:,1)\n",
      "(,4)\n",
      "(documentation,1)\n",
      "(projet,,1)\n",
      "(donné,1)\n",
      "(Scala.,1)\n",
      "(tout,1)\n",
      "(cours,1)\n",
      "(qu'on,1)\n",
      "(Data.,1)\n",
      "(Ce,1)\n",
      "(trouverez,2)\n",
      "(sont,1)\n",
      "(sa,1)\n",
      "(ressources,2)\n",
      "(rassemble,1)\n",
      "(mais,1)\n",
      "(autres,1)\n",
      "(également,2)\n",
      "(:,1)\n",
      "(lire,1)\n",
      "(séances,1)\n",
      "(toutes,1)\n",
      "(Spécialisé,1)\n",
      "(TP.,1)\n",
      "(sur,4)\n",
      "(est,2)\n",
      "([TP_3_machine_learning_avec_spark.md](TP_3_machine_learning_avec_spark.md),1)\n",
      "(fichiers.,1)\n",
      "(la,1)\n",
      "(Vous,2)\n",
      "(ici,1)\n",
      "(Spark,4)\n",
      "(ces,1)\n",
      "(que,1)\n",
      "(créer,1)\n",
      "(Télécom,1)\n",
      "(généralités,1)\n",
      "([spark_notes.md](spark_notes.md),1)\n",
      "(a,1)\n",
      "(travaux,1)\n",
      "(de,3)\n",
      "(y,1)\n",
      "(le,1)\n",
      "(recommandé,1)\n",
      "(et,4)\n",
      "(#,1)\n",
      "([TP_1_spark_shell_et_word_count.md](TP_1_spark_shell_et_word_count.md),1)\n",
      "(faire,1)\n",
      "([TP_2_projet_et_pre_processings.md](TP_2_projet_et_pre_processings.md),1)\n",
      "(3,1)\n",
      "(décrit,1)\n",
      "(un,1)\n",
      "(du,1)\n",
      "(-,3)\n",
      "(partie,1)\n",
      "(quelques,1)\n",
      "(tourner,1)\n",
      "([setup.md](setup.md),1)\n",
      "(Il,2)\n",
      "(qui,3)\n",
      "(comment,1)\n",
      "(fichiers,3)\n"
     ]
    }
   ],
   "source": [
    "rdd.map(word => (word, 1)).reduceByKey((i, j) => i + j).collect.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le job a mis cette fois beaucoup moins de temps car la base de données est en mode persistante. Le premier job avait mis **1,0 seconde** à s'éxécuter, alors que le second n'a mis que **0,1 seconde**.\n",
    "\n",
    "\n",
    "Persister un RDD/DataFrame prend évidemment de la place en mémoire/disque. Quand on n'en a plus besoin, il faut l'*unpersister* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res28: rdd.type = MapPartitionsRDD[85] at flatMap at <console>:27\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.unpersist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
