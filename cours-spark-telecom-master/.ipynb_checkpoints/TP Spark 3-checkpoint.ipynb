{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "# <center>TP Spark 3 : Machine learning avec Spark</center>\n",
    "-------------------\n",
    "\n",
    "## Chargement du DataFrame\n",
    "\n",
    "Charger le DataFrame obtenu à la fin du TP 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://137.194.91.204:4040\n",
       "SparkContext available as 'sc' (version = 2.4.4, master = local[*], app id = local-1573822096865)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataframe\n",
      "+--------------+--------------------+--------------------+------+--------------------+------------+--------+---------+-------------------+-------------------+-------------------+-------------+-----------+--------------------+\n",
      "|    project_id|                name|                desc|  goal|            keywords|final_status|country2|currency2|          deadline2|        created_at2|       launched_at2|days_campaign|hours_prepa|                text|\n",
      "+--------------+--------------------+--------------------+------+--------------------+------------+--------+---------+-------------------+-------------------+-------------------+-------------+-----------+--------------------+\n",
      "| kkst471421639|american options ...|looking to create...|100000|american-options-...|           0|      US|      USD|2014-11-15 17:31:27|2014-10-10 21:23:58|2014-10-16 17:31:27|           30|    140.125|american options ...|\n",
      "|kkst1098019088|iheadbones bone c...|wireless bluetoot...| 20000|iheadbones-bone-c...|           0|      US|      USD|2014-11-15 17:37:42|2012-08-30 23:07:05|2014-10-16 17:37:42|           30|   18642.51|iheadbones bone c...|\n",
      "|kkst1719475563| the fridge magazine|the fridge is a n...|   700| the-fridge-magazine|           0|      US|      USD|2014-11-15 17:41:58|2014-09-02 17:35:56|2014-09-16 17:41:58|           60|    336.101|the fridge magazi...|\n",
      "| kkst564469925|support new men's...|it s been over 10...| 12800|support-new-mens-...|           0|      US|      USD|2014-11-15 17:44:42|2014-09-07 19:32:20|2014-09-16 17:44:42|           60|    214.206|support new men's...|\n",
      "|kkst1213811673|             can('t)|a psychological h...|  1500|              cant-0|           0|      US|      USD|2014-11-15 17:57:32|2014-11-04 00:25:15|2014-11-05 17:57:32|           10|     41.538|can('t) a psychol...|\n",
      "| kkst604127707|     fragmented fate|experience a mode...| 60000|     fragmented-fate|           0|      US|      USD|2014-11-15 18:00:22|2014-10-15 06:22:04|2014-10-16 18:00:22|           30|     35.638|fragmented fate e...|\n",
      "| kkst152922918|transport (suspen...|help ons met een ...| 10000|           transport|           0|      NL|      EUR|2014-11-15 18:19:00|2014-10-15 18:37:17|2014-10-16 18:19:00|           30|     23.695|transport (suspen...|\n",
      "|  kkst15847426|the secret life o...|a stage show usin...|  1000|the-secret-life-o...|           0|      GB|      GBP|2014-11-15 18:53:48|2014-10-01 19:03:22|2014-10-16 18:53:48|           30|    359.841|the secret life o...|\n",
      "|kkst1019043720|         cc survival|deception. diplom...|  1000|         cc-survival|           0|      GB|      GBP|2014-11-15 19:00:00|2014-10-22 19:26:51|2014-10-28 18:59:58|           18|    144.552|cc survival decep...|\n",
      "| kkst830969808|the best protein ...|all natural  no a...|145000|the-best-protein-...|           0|      US|      USD|2014-11-15 19:04:37|2014-10-11 00:38:27|2014-10-16 19:04:37|           30|    138.436|the best protein ...|\n",
      "| kkst711744335|      paradise falls|paradise falls is...|  4000|      paradise-falls|           0|      GB|      GBP|2014-11-15 19:07:39|2014-10-12 12:51:29|2014-10-18 19:07:39|           28|    150.269|paradise falls pa...|\n",
      "|kkst1489126767|the chalet woodsh...|you are awesome a...| 10000|the-chalet-woodsh...|           1|      US|      USD|2014-11-15 19:08:36|2014-09-12 04:11:03|2014-10-16 19:08:36|           30|    830.959|the chalet woodsh...|\n",
      "|kkst1436642853|vagabond mobile g...|vagabond is a ser...|  1500|vagabond-mobile-g...|           0|      GB|      GBP|2014-11-15 19:09:34|2014-03-11 11:45:34|2014-10-31 18:09:34|           15|     5622.4|vagabond mobile g...|\n",
      "| kkst788220752|southern shakespe...|bringing free sha...|  7500|southern-shakespe...|           1|      US|      USD|2014-11-15 19:10:43|2014-09-29 15:37:20|2014-10-16 19:10:43|           30|    411.556|southern shakespe...|\n",
      "|kkst2055681419|leviathan: montau...|creating portrait...|  3000|leviathan-montauk...|           1|      US|      USD|2014-11-15 19:12:00|2014-09-10 23:21:47|2014-10-16 20:04:51|           30|    860.718|leviathan: montau...|\n",
      "| kkst892111701|     the candle tray|hand made candle ...|  5000|     the-candle-tray|           0|      US|      USD|2014-11-15 19:14:00|2014-10-10 19:12:37|2014-10-16 18:10:13|           30|     142.96|the candle tray h...|\n",
      "| kkst937888094|            sun skin|the mission is to...|  2500|            sun-skin|           0|      US|      USD|2014-11-15 19:22:04|2014-06-12 00:07:36|2014-09-16 19:22:04|           60|   2323.241|sun skin the miss...|\n",
      "|kkst1864352284|7sonic debut stud...|making noise usin...| 31500|7sonic-debut-stud...|           0|      US|      USD|2014-11-15 19:23:12|2014-09-14 01:12:12|2014-09-16 19:23:12|           60|     66.183|7sonic debut stud...|\n",
      "| kkst607454107|the hades pit: a ...|a young woman emb...|310000|the-hades-pit-an-...|           0|      GB|      GBP|2014-11-15 19:41:45|2013-10-22 13:19:27|2014-10-06 19:41:45|           40|   8382.372|the hades pit: a ...|\n",
      "|kkst1553225242|the fitness refinery|our dream is to c...|  3000|the-fitness-refinery|           0|      GB|      GBP|2014-11-15 19:51:34|2014-10-08 22:23:20|2014-10-16 19:51:34|           30|    189.471|the fitness refin...|\n",
      "+--------------+--------------------+--------------------+------+--------------------+------------+--------+---------+-------------------+-------------------+-------------------+-------------+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.DataFrame\n",
       "df: org.apache.spark.sql.DataFrame = [project_id: string, name: string ... 12 more fields]\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.DataFrame\n",
    "\n",
    "val df: DataFrame = spark\n",
    "  .read\n",
    "  .option(\"header\", true) // utilise la première ligne du (des) fichier(s) comme header\n",
    "  .option(\"inferSchema\", \"true\") // pour inférer le type de chaque colonne (Int, String, etc.)\n",
    "  .parquet(\"/home/p5hngk/Downloads/GitHub/INF_729---Introduction_au_framework_Hadoop/cours-spark-telecom-master/data/prepared_trainingset\")\n",
    "\n",
    "println(\"Training Dataframe\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.{CountVectorizer, IDF, OneHotEncoderEstimator, RegexTokenizer, StringIndexer}\n",
       "import org.apache.spark.ml.feature.VectorAssembler\n",
       "import org.apache.spark.ml.classification.LogisticRegression\n",
       "import org.apache.spark.ml.feature.StopWordsRemover\n",
       "import org.apache.spark.ml.Pipeline\n",
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
       "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit, TrainValidationSplitModel}\n"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{CountVectorizer, IDF, OneHotEncoderEstimator, RegexTokenizer, StringIndexer}\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.feature.StopWordsRemover\n",
    "\n",
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit, TrainValidationSplitModel}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------\n",
    "## Utilisation des données textuelles\n",
    "Les textes ne sont pas utilisables tels quels par les algorithmes parce qu’ils ont besoin de données numériques, en particulier pour les calculs d’erreurs et d’optimisation. On veut donc convertir la colonne \"text\" en données numériques. Une façon très répandue de faire cela est d’appliquer l’algorithme [TF-IDF](https://spark.apache.org/docs/latest/ml-features.html#tf-idf).\n",
    "\n",
    "### Stage 1 : récupérer les mots des textes\n",
    "La première étape est de séparer les textes en mots (ou tokens) avec un tokenizer. Construire le premier stage du pipeline de la façon suivante :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer: org.apache.spark.ml.feature.RegexTokenizer = regexTok_f0d7cd885ec5\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tokenizer = new RegexTokenizer()\n",
    "  .setPattern(\"\\\\W+\")\n",
    "  .setGaps(true)\n",
    "  .setInputCol(\"text\")\n",
    "  .setOutputCol(\"tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2 : retirer les stops words\n",
    "\n",
    "On veut retirer les [stop words](https://en.wikipedia.org/wiki/Stop_words) pour ne pas encombrer le modèle avec des mots qui ne véhiculent pas de sens. On va donc créer le 2ème stage avec la classe `StopWordsRemover`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stopWordsRemover: org.apache.spark.ml.feature.StopWordsRemover = stopWords_e5c52a1d88c2\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val stopWordsRemover = new StopWordsRemover()\n",
    "  .setInputCol(\"tokens\")\n",
    "  .setOutputCol(\"filtered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 3 : computer la partie TF\n",
    "La partie TF de TF-IDF est faite avec la classe `CountVectorizer`. Lire la [doc](https://spark.apache.org/docs/latest/ml-features.html#tf-idf) pour plus d'info sur TF-IDF et son implémentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "countVectorizedModel: org.apache.spark.ml.feature.CountVectorizer = cntVec_e5f43fbe87b3\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val countVectorizedModel = new CountVectorizer()\n",
    "      .setInputCol(\"filtered\")\n",
    "      .setOutputCol(\"vectorized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 4 : computer la partie IDF\n",
    "Implémentons la partie IDF avec en output une colonne ***tfidf***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "idf: org.apache.spark.ml.feature.IDF = idf_5c82ac10ba3c\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val idf = new IDF()\n",
    "      .setInputCol(\"vectorized\")\n",
    "      .setOutputCol(\"tfidf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------\n",
    "## Conversion des variables catégorielles en variables numériques\n",
    "\n",
    "Les colonnes ***country2*** et ***currency2*** sont des variables catégorielles (qui ne prennent qu’un ensemble limité de valeurs, ces valeurs n'ayant, ici, aucune notion d'ordre entre elles), par opposition aux variables continues comme ***goal*** ou ***hours_prepa*** qui peuvent prendre n’importe quelle valeur réelle positive. Ici les catégories sont indiquées par une chaîne de charactères, e.g. \"US\" ou \"EUR\". On veut convertir ces classes en quantités numériques.\n",
    "\n",
    "### Stage 5 : convertir ***country2*** en quantités numériques\n",
    "\n",
    "Nous allons mettre les résultats dans une colonne ***country_indexed***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stringIndexer: org.apache.spark.ml.feature.StringIndexer = strIdx_8b6ab1865213\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val stringIndexer = new StringIndexer()\n",
    "    .setInputCol(\"country2\")\n",
    "    .setOutputCol(\"country_indexed\")\n",
    "    .setHandleInvalid(\"skip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 6 : convertir ***currency2*** en quantités numériques\n",
    "\n",
    "Nous allons mettre les résultats dans une colonne ***currency_indexed***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stringIndexer2: org.apache.spark.ml.feature.StringIndexer = strIdx_bc8d4a0514db\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val stringIndexer2 = new StringIndexer()\n",
    "      .setInputCol(\"currency2\")\n",
    "      .setOutputCol(\"currency_indexed\")\n",
    "      .setHandleInvalid(\"skip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 7 et 8 : One-Hot encoder ces deux catégories \n",
    "Transformons ces deux catégories avec un \"one-hot encoder\" en créant les colonnes ***country_onehot*** et ***currency_onehot***. Une page [Quora](https://www.quora.com/What-is-one-hot-encoding-and-when-is-it-used-in-data-science) sur le one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "oneHotEncoder: org.apache.spark.ml.feature.OneHotEncoderEstimator = oneHotEncoder_9ad308b19850\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val oneHotEncoder = new OneHotEncoderEstimator()\n",
    "      .setInputCols(Array(\"country_indexed\", \"currency_indexed\"))\n",
    "      .setOutputCols(Array(\"country_onehot\", \"currency_onehot\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "## Mettre les données sous une forme utilisable par Spark.ML\n",
    "\n",
    "\n",
    "La plupart des algorithmes de machine learning dans Spark requièrent que les colonnes utilisées en input du modèle (les features du modèle) soient regroupées dans une seule colonne qui contient des vecteurs. On veut donc passer de :\n",
    "\n",
    "|Feature A|Feature B|Feature C|Label|\n",
    "|:---:|:---:|:---:|:---:|\n",
    "|0.5|1|3.5|0|\n",
    "|0.6|1|1.2|1|\n",
    "\n",
    "à\n",
    "\n",
    "| Features | Label |\n",
    "|:---:|:---:|\n",
    "|(0.5, 1, 3.5)|0|\n",
    "|(0.6, 1, 1.2)|1|\n",
    "\n",
    "### Stage 9 : assembler tous les features en un unique vecteur\n",
    "\n",
    "Assemblons les features ***tfidf***, ***days_campaign***, ***hours_prepa***, ***goal***, ***country_onehot***, et ***currency_onehot*** dans une seule colonne ***features***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT FEATURES\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "vectorAssembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_672416c38e7f\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val vectorAssembler = new VectorAssembler()\n",
    "      .setInputCols(Array(\"tfidf\", \"days_campaign\", \"hours_prepa\", \"goal\", \"country_onehot\", \"currency_onehot\"))\n",
    "      .setOutputCol(\"features\")\n",
    "\n",
    "println(\"OUTPUT FEATURES\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 10 : créer/instancier le modèle de classification\n",
    "\n",
    "Le classifieur que nous utilisons est une [régression logistique](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.classification.LogisticRegression) avec une régularisation dans la fonction de coût qui permet de pénaliser les features les moins fiables pour la classification.\n",
    "\n",
    "On la définit de la façon suivante :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lr: org.apache.spark.ml.classification.LogisticRegression = logreg_e2b134433876\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lr = new LogisticRegression()\n",
    "  .setElasticNetParam(0.0)\n",
    "  .setFitIntercept(true)\n",
    "  .setFeaturesCol(\"features\")\n",
    "  .setLabelCol(\"final_status\")\n",
    "  .setStandardization(true)\n",
    "  .setPredictionCol(\"predictions\")\n",
    "  .setRawPredictionCol(\"raw_predictions\")\n",
    "  .setThresholds(Array(0.7, 0.3))\n",
    "  .setTol(1.0e-6)\n",
    "  .setMaxIter(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------\n",
    "## Création du Pipeline\n",
    "\n",
    "Créons maintenant le [pipeline](https://spark.apache.org/docs/latest/ml-pipeline.html#ml-pipelines) en assemblant les 10 stages définis précédemment, dans le bon ordre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stages10: Array[org.apache.spark.ml.PipelineStage with org.apache.spark.ml.util.DefaultParamsWritable{def copy(extra: org.apache.spark.ml.param.ParamMap): org.apache.spark.ml.PipelineStage with org.apache.spark.ml.util.DefaultParamsWritable{def copy(extra: org.apache.spark.ml.param.ParamMap): org.apache.spark.ml.PipelineStage with org.apache.spark.ml.util.DefaultParamsWritable{def copy(extra: org.apache.spark.ml.param.ParamMap): org.apache.spark.ml.PipelineStage with org.apache.spark.ml.util.DefaultParamsWritable}}}] = Array(regexTok_f0d7cd885ec5, stopWords_e5c52a1d88c2, cntVec_e5f43fbe87b3, idf_5c82ac10ba3c, strIdx_8b6ab1865213, strIdx_bc8d4a0514db, oneHotEncoder_9ad308b19850, vecAssembler_672416c38e7f, logreg_e2b134433876)\n",
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_c014b6d0b36d\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val stages10 = Array(tokenizer, stopWordsRemover, countVectorizedModel, idf, stringIndexer, stringIndexer2, oneHotEncoder, vectorAssembler, lr)\n",
    "    val pipeline = new Pipeline().setStages(stages10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraînement, test, et sauvegarde du modèle\n",
    "\n",
    "### Split des données en training et test sets\n",
    "\n",
    "On veut séparer les données aléatoirement en un training set (90% des données) qui servira à l’entraînement du modèle et un test set (10% des données) qui servira à tester la qualité du modèle sur des données que le modèle n’a jamais vues lors de son entraînement. Cette phase est nécessaire pour avoir des résultats non-biaisés sur la pertinence du modèle obtenu.\n",
    "\n",
    "Créons un DataFrame nommé **training** et un autre nommé **test** à partir du DataFrame chargé initialement de façon à le séparer en training et test sets dans les proportions 90%, 10% respectivement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [project_id: string, name: string ... 12 more fields]\n",
       "test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [project_id: string, name: string ... 12 more fields]\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(training, test) = df.randomSplit(Array(0.9, 0.1), seed = 1991)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entraînement du modèle\n",
    "\n",
    "Entraînons notre modèle via le pipeline que nous avons créé puis sauvegardons-le."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+--------------------+-------+--------------------+------------+--------+---------+-------------------+-------------------+-------------------+-------------+-----------+--------------------+\n",
      "|    project_id|                name|                desc|   goal|            keywords|final_status|country2|currency2|          deadline2|        created_at2|       launched_at2|days_campaign|hours_prepa|                text|\n",
      "+--------------+--------------------+--------------------+-------+--------------------+------------+--------+---------+-------------------+-------------------+-------------------+-------------+-----------+--------------------+\n",
      "|kkst1000004038|production elvis ...|a live stage prod...|  10000|production-elvis-...|           1|      US|      USD|2014-11-21 03:35:00|2014-09-26 02:03:58|2014-09-28 11:13:10|           54|     57.153|production elvis ...|\n",
      "|kkst1000217634|haven's story: ti...|we proudly presen...|    150|havens-story-time...|           0|      NL|      EUR|2014-12-31 14:11:40|2014-11-24 12:07:21|2014-12-01 14:11:40|           30|    170.072|haven's story: ti...|\n",
      "|kkst1000230071|northern lights o...|northern lights o...|  70000|northern-lights-o...|           0|      NO|      NOK|2014-12-13 21:06:35|2014-10-12 18:07:40|2014-11-13 21:06:35|           30|    771.982|northern lights o...|\n",
      "|kkst1000257563|             the uvu|the uvu is an onl...|1820500|             the-uvu|           0|      US|      USD|2014-12-22 22:03:24|2014-10-16 01:23:25|2014-11-22 22:03:24|           30|    909.666|the uvu the uvu i...|\n",
      "|kkst1000333671|happy birthday, m...|happy birthday  m...|  25000|happy-birthday-ma...|           0|      US|      USD|2014-08-15 15:59:12|2014-07-10 19:05:30|2014-07-16 15:59:12|           30|    140.895|happy birthday, m...|\n",
      "|kkst1000374001|breakfast at tifi...|breakfast at tifi...|  70000|breakfast-at-tifi...|           0|      US|      USD|2015-01-01 01:00:00|2014-09-16 23:21:38|2014-12-11 23:14:02|           21|   2064.873|breakfast at tifi...|\n",
      "|kkst1000392220|lúnico oslo stree...|our 1964 piaggio ...|  55000|lunico-oslo-stree...|           0|      NO|      NOK|2014-12-24 16:34:01|2014-09-21 17:34:07|2014-10-30 15:34:01|           55|    934.998|lúnico oslo stree...|\n",
      "|kkst1000521061|pictures across t...|a drive from va t...|   3000|pictures-across-t...|           0|      US|      USD|2014-09-05 21:01:41|2014-08-05 20:42:27|2014-08-06 21:01:41|           30|     24.321|pictures across t...|\n",
      "|kkst1000524949|juneau's giant pu...|everyone s an art...|   8500|juneaus-giant-pup...|           0|      US|      USD|2014-12-22 02:00:00|2014-11-05 16:49:44|2014-11-12 08:42:26|           40|    159.878|juneau's giant pu...|\n",
      "|kkst1000537760|reimagined: class...|reimagined will b...|   6000|reimagined-classi...|           0|      US|      USD|2014-09-06 21:19:06|2014-03-20 22:33:36|2014-08-07 21:19:06|           30|   3357.758|reimagined: class...|\n",
      "+--------------+--------------------+--------------------+-------+--------------------+------------+--------+---------+-------------------+-------------------+-------------------+-------------+-----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Model 1 was fit using parameters: {\n",
      "\tpipeline_c014b6d0b36d-stages: [Lorg.apache.spark.ml.PipelineStage;@ba48576\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.ml.PipelineModel = pipeline_c014b6d0b36d\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model = pipeline.fit(training)\n",
    "println(s\"Model 1 was fit using parameters: ${model.parent.extractParamMap}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test du modèle\n",
    "\n",
    "https://spark.apache.org/docs/latest/ml-pipeline.html\n",
    "\n",
    "https://spark.apache.org/docs/latest/ml-tuning.html\n",
    "\n",
    "* Appliquons le modèle aux données de test. Mettons les résultats dans le DataFrame `dfWithSimplePredictions`.\n",
    "\n",
    "* Affichons\n",
    "```scala\n",
    "dfWithSimplePredictions.groupBy(\"final_status\", \"predictions\").count.show()\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+-----+\n",
      "|final_status|predictions|count|\n",
      "+------------+-----------+-----+\n",
      "|           1|        0.0| 1810|\n",
      "|           0|        0.0| 5016|\n",
      "|           1|        1.0| 1600|\n",
      "|           0|        1.0| 2376|\n",
      "+------------+-----------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dfWithSimplePredictions: org.apache.spark.sql.DataFrame = [project_id: string, name: string ... 24 more fields]\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfWithSimplePredictions = model.transform(test)\n",
    "\n",
    "dfWithSimplePredictions.groupBy(\"final_status\", \"predictions\").count.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Affichons le [*f1-score*](https://en.wikipedia.org/wiki/F1_score) du modèle sur les données de test (cette métrique s'obtient via [MulticlassClassificationEvaluator](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_d6f6bd8301b5\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "    .setMetricName(\"f1\")\n",
    "    .setLabelCol(\"final_status\")\n",
    "    .setPredictionCol(\"predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le f1-score est de 0.6196139660457709\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "f1score: Double = 0.6196139660457709\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val f1score = evaluator.evaluate(dfWithSimplePredictions)\n",
    "\n",
    "println(\"Le f1-score est de \" + f1score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------\n",
    "## Réglage des hyper-paramètres (a.k.a. tuning) du modèle\n",
    "\n",
    "La façon de procéder présentée plus haut permet rapidement d'entraîner un modèle et d'avoir une mesure de sa performance. Mais que se passe-t-il si l'ont souhaite utiliser 300 itérations au maximum plutôt que 50 (i.e. la ligne `.setMaxIter(50)`) ? Si l'on souhaite modifier le paramètre de régularisation du modèle ? Si l'on souhaite modifier le paramètre *minDF* de la classe `CountVectorizer` (qui permet de ne prendre que les mots apparaissant dans au moins minDF documents) ? Il faudrait à chaque fois modifier le(s) paramètre(s) à la main, ré-entraîner le modèle, re-calculer la performance du modèle obtenu sur l'ensemble de test, puis finalement choisir le meilleur modèle (i.e. celui avec la meilleure performance sur les données de test) parmi tous ces modèles entraînés. C'est ce qu'on appelle le réglage des hyper-paramètres ou encore tuning du modèle. Et c'est fastidieux.\n",
    "\n",
    "La plupart des algorithmes de machine learning possèdent des hyper-paramètres, par exemple le nombre de couches et de neurones dans un réseau de neurones, le nombre d’arbres et leur profondeur maximale dans les random forests, etc. Qui plus est, comme mentionné précédemment avec le paramètre *minDF* de la classe `CountVectorizer`, on peut également se retrouver avec des hyper-paramètres au niveau des stages de préprocessing. L'objectif est donc de trouver la meilleure combinaison possible de tous ces hyper-paramètres.\n",
    "\n",
    "### Grid search\n",
    "\n",
    "Une des techniques pour régler automatiquement les hyper-paramètres est la *grid search* qui consiste à :\n",
    "- créer une grille de valeurs à tester pour les hyper-paramètres\n",
    "- en chaque point de la grille\n",
    "    - séparer le training set en un ensemble de training (70%) et un ensemble de validation (30%)\n",
    "    - entraîner un modèle sur le training set\n",
    "    - calculer l’erreur du modèle sur le validation set\n",
    "- sélectionner le point de la grille (<=> garder les valeurs d’hyper-paramètres de ce point) où l’erreur de validation est la plus faible i.e. là où le modèle a le mieux appris\n",
    "\n",
    "Pour la régularisation de notre régression logistique on veut tester les valeurs de 10e-8 à 10e-2 par pas de 2.0 en échelle logarithmique (on veut tester les valeurs 10e-8, 10e-6, 10e-4 et 10e-2).\n",
    "Pour le paramètre minDF de CountVectorizer on veut tester les valeurs de 55 à 95 par pas de 20. \n",
    "En chaque point de la grille on veut utiliser 70% des données pour l’entraînement et 30% pour la validation.\n",
    "On veut utiliser le *f1-score* pour comparer les différents modèles en chaque point de la grille.\n",
    "\n",
    "Préparons la grid-search pour satisfaire les conditions explicitées ci-dessus puis lançons la grid-search sur le dataset \"training\" préparé précédemment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tcntVec_e5f43fbe87b3-minDF: 55.0,\n",
       "\tlogreg_e2b134433876-regParam: 1.0E-7\n",
       "}, {\n",
       "\tcntVec_e5f43fbe87b3-minDF: 75.0,\n",
       "\tlogreg_e2b134433876-regParam: 1.0E-7\n",
       "}, {\n",
       "\tcntVec_e5f43fbe87b3-minDF: 95.0,\n",
       "\tlogreg_e2b134433876-regParam: 1.0E-7\n",
       "}, {\n",
       "\tcntVec_e5f43fbe87b3-minDF: 55.0,\n",
       "\tlogreg_e2b134433876-regParam: 1.0E-5\n",
       "}, {\n",
       "\tcntVec_e5f43fbe87b3-minDF: 75.0,\n",
       "\tlogreg_e2b134433876-regParam: 1.0E-5\n",
       "}, {\n",
       "\tcntVec_e5f43fbe87b3-minDF: 95.0,\n",
       "\tlogreg_e2b134433876-regParam: 1.0E-5\n",
       "}, {\n",
       "\tcntVec_e5f43fbe87b3-minDF: 55.0,\n",
       "\tlogreg_e2b134433876-regParam: 0.001\n",
       "}, {\n",
       "\tcntVec_e5f43fbe87b3-minDF: 75.0,\n",
       "\tlogreg_e2b134433876-regParam: 0.001\n",
       "}, {\n",
       "\tcntVec_e5f43fbe87b3-minDF: 95.0,\n",
       "\tlogreg_e2b134433876-regParam: 0.001\n",
       "}, {\n",
       "\tcntVec_e5f43fbe87b3-minDF: 55.0,\n",
       "\tlogreg_e2..."
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val paramGrid = new ParamGridBuilder()\n",
    "    .addGrid(lr.regParam, Array(10e-8, 10e-6, 10e-4, 10e-2))\n",
    "    .addGrid(countVectorizedModel.minDF, Array(55.0, 75.0, 95.0))\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainValidationSplit: org.apache.spark.ml.tuning.TrainValidationSplit = tvs_fed7ded53f02\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//  TrainValidationSplit requiert un estimateur, un set d'estimateur ParamMaps, et un Evaluator.\n",
    "val trainValidationSplit = new TrainValidationSplit()\n",
    "    .setEstimator(pipeline)\n",
    "    .setEvaluator(evaluator)\n",
    "    .setEstimatorParamMaps(paramGrid)\n",
    "    .setTrainRatio(0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrainement du modèle avec l'échantillon training\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "validationModel: org.apache.spark.ml.tuning.TrainValidationSplitModel = tvs_fed7ded53f02\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Entrainement du modèle avec l'échantillon training\n",
    "println(\"Entrainement du modèle avec l'échantillon training\")\n",
    "val validationModel = trainValidationSplit.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test du modèle\n",
    "\n",
    "On a vu que pour évaluer de façon non biaisée la pertinence du modèle obtenu, il fallait le tester sur des données qu'il n'avait jamais vues pendant son entraînement. Ça vaut également pour les données utilisées pour sélectionner le meilleur modèle de la grid search (training et validation)! C’est pour cela que nous avons construit le dataset de test que nous avons laissé de côté jusque là.\n",
    "\n",
    "* Appliquons le meilleur modèle trouvé avec la grid-search aux données de test. Mettons les résultats dans le DataFrame `dfWithPredictions`. Affichons le f1-score du modèle sur les données de test.\n",
    "\n",
    "* Affichons\n",
    "```scala\n",
    "dfWithPredictions.groupBy(\"final_status\", \"predictions\").count.show()\n",
    "```\n",
    "\n",
    "\n",
    "* Sauvegardons le modèle entraîné pour pouvoir le réutiliser plus tard.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+-----+\n",
      "|final_status|predictions|count|\n",
      "+------------+-----------+-----+\n",
      "|           1|        0.0| 1049|\n",
      "|           0|        0.0| 4493|\n",
      "|           1|        1.0| 2361|\n",
      "|           0|        1.0| 2899|\n",
      "+------------+-----------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dfWithPredictions: org.apache.spark.sql.DataFrame = [features: vector, final_status: int ... 1 more field]\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfWithPredictions = validationModel.transform(test).select(\"features\",\"final_status\",\"predictions\")\n",
    "\n",
    "dfWithPredictions.groupBy(\"final_status\", \"predictions\").count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+-----+\n",
      "|final_status|predictions|count|\n",
      "+------------+-----------+-----+\n",
      "|           1|        0.0| 1049|\n",
      "|           0|        0.0| 4493|\n",
      "|           1|        1.0| 2361|\n",
      "|           0|        1.0| 2899|\n",
      "+------------+-----------+-----+\n",
      "\n",
      "F1 Score est 0.647367360180229\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "score: Double = 0.647367360180229\n"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val score = evaluator.evaluate(dfWithPredictions)\n",
    "\n",
    "dfWithPredictions.groupBy(\"final_status\",\"predictions\").count.show()\n",
    "\n",
    "println(\"F1 Score est \" + score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Saving model\n",
    "\n",
    "validationModel.save(\"/home/p5hngk/Downloads/GitHub/INF_729---Introduction_au_framework_Hadoop/cours-spark-telecom-master/model/LogisticRegression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Remarque :*** On peut également évaluer la précision avec le modèle suivant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision obtenue : 0.6345121273838178\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "evaluator_acc: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_3717f023f7bb\n",
       "accuracy: Double = 0.6345121273838178\n"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Evaluer la precision (accuracy)\n",
    "    val evaluator_acc = new MulticlassClassificationEvaluator()\n",
    "      .setLabelCol(\"final_status\")\n",
    "      .setPredictionCol(\"predictions\")\n",
    "      .setMetricName(\"accuracy\")\n",
    "\n",
    "    // obtention de la mesure de performance\n",
    "    val accuracy = evaluator_acc.evaluate(dfWithPredictions)\n",
    "    println(\"Precision obtenue : \" + accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------\n",
    "----------------------------------------\n",
    "------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res15: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tcntVec_e5f43fbe87b3-minDF: 55.0,\n",
       "\tlogreg_e2b134433876-regParam: 1.0E-7\n",
       "}, {\n",
       "\tcntVec_e5f43fbe87b3-minDF: 75.0,\n",
       "\tlogreg_e2b134433876-regParam: 1.0E-7\n",
       "}, {\n",
       "\tcntVec_e5f43fbe87b3-minDF: 95.0,\n",
       "\tlogreg_e2b134433876-regParam: 1.0E-7\n",
       "}, {\n",
       "\tcntVec_e5f43fbe87b3-minDF: 55.0,\n",
       "\tlogreg_e2b134433876-regParam: 1.0E-5\n",
       "}, {\n",
       "\tcntVec_e5f43fbe87b3-minDF: 75.0,\n",
       "\tlogreg_e2b134433876-regParam: 1.0E-5\n",
       "}, {\n",
       "\tcntVec_e5f43fbe87b3-minDF: 95.0,\n",
       "\tlogreg_e2b134433876-regParam: 1.0E-5\n",
       "}, {\n",
       "\tcntVec_e5f43fbe87b3-minDF: 55.0,\n",
       "\tlogreg_e2b134433876-regParam: 0.001\n",
       "}, {\n",
       "\tcntVec_e5f43fbe87b3-minDF: 75.0,\n",
       "\tlogreg_e2b134433876-regParam: 0.001\n",
       "}, {\n",
       "\tcntVec_e5f43fbe87b3-minDF: 95.0,\n",
       "\tlogreg_e2b134433876-regParam: 0.001\n",
       "}, {\n",
       "\tcntVec_e5f43fbe87b3-minDF: 55.0,\n",
       "\tlogreg_e2b134..."
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" A SUPPRIMER \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "validationModel.getEstimatorParamMaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res19: String =\n",
       "estimator: estimator for selection (current: pipeline_c014b6d0b36d)\n",
       "estimatorParamMaps: param maps for the estimator (current: [Lorg.apache.spark.ml.param.ParamMap;@1c610ab0)\n",
       "evaluator: evaluator used to select hyper-parameters that maximize the validated metric (current: mcEval_d6f6bd8301b5)\n",
       "seed: random seed (default: -1772833110)\n",
       "trainRatio: ratio between training set and validation set (>= 0 && <= 1) (default: 0.75, current: 0.7)\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "43: error: not found: type PipelineModel",
     "output_type": "error",
     "traceback": [
      "<console>:43: error: not found: type PipelineModel",
      "       validationModel.bestModel.asInstanceOf[PipelineModel].stages(10).asInstanceOf[GBTClassificationModel].extractParamMap()",
      "                                              ^",
      "<console>:43: error: not found: type GBTClassificationModel",
      "       validationModel.bestModel.asInstanceOf[PipelineModel].stages(10).asInstanceOf[GBTClassificationModel].extractParamMap()",
      "                                                                                     ^",
      ""
     ]
    }
   ],
   "source": [
    "validationModel.bestModel.asInstanceOf[PipelineModel].stages(10).asInstanceOf[GBTClassificationModel].extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res43: String = pipeline_c014b6d0b36d\n"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validationModel.bestModel.toString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
